{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "!pip install recnn\n",
    "!pip install torch_optimizer\n",
    "!pip install tensorboard\n",
    "!pip install sklearn\n",
    "!pip install jupyterthemes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T00:00:08.842442Z",
     "end_time": "2023-06-15T00:00:14.316938Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import misc\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='grade3')\n",
    "sys.path.append(\"../../\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:41.680981Z",
     "end_time": "2023-06-15T14:01:34.422488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def get_tensor(x, y, z):\n",
    "    multiply_tensor = torch.zeros((x * z, y * z, x * y), dtype=torch.float32)\n",
    "\n",
    "    for i in range(z):\n",
    "        for j in range(x):\n",
    "            for k in range(y):\n",
    "                multiply_tensor[i * x + j][i * y + k][j + k * x] = 1\n",
    "\n",
    "    final_tensor = torch.tensor(multiply_tensor)\n",
    "    return final_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:41.683106Z",
     "end_time": "2023-06-15T14:01:34.426366Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/7g_7wfqd5hj9qmks2g8q5l200000gn/T/ipykernel_49000/2081598127.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_tensor = torch.tensor(multiply_tensor)\n",
      "/var/folders/y7/7g_7wfqd5hj9qmks2g8q5l200000gn/T/ipykernel_49000/2081598127.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_tensor = torch.tensor(multiply_tensor)\n"
     ]
    }
   ],
   "source": [
    "lib = {}\n",
    "for a in range(1, 10):\n",
    "    for b in range(1, 10):\n",
    "        for c in range(1, 10):\n",
    "            lib.update({(a, b, c): get_tensor(a, b, c)})\n",
    "\n",
    "with open('tensor_holder\\\\tensors', 'wb') as f:\n",
    "        pickle.dump(lib, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.048841Z",
     "end_time": "2023-06-15T14:01:34.785860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cuda = torch.device('cpu')\n",
    "\n",
    "# ---\n",
    "frame_size = 10\n",
    "batch_size = 25\n",
    "n_epochs   = 100\n",
    "plot_every = 30\n",
    "step       = 0\n",
    "# ---\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "with open('tensor_holder\\\\tensors', 'rb') as f:\n",
    "    multiply_tensors = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.052281Z",
     "end_time": "2023-06-15T14:01:34.787930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.m = self.shape[0]\n",
    "        self.n = self.shape[1]\n",
    "        self.d = self.shape[2]\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = torch.clone(multiply_tensors[self.shape]).to(torch.float32)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform action on the environment\n",
    "        # Update the state based on the action\n",
    "        # Compute the reward and done flag\n",
    "        summ = torch.outer(torch.tensor(action[0]), torch.tensor(action[1]))\n",
    "        for j in range(4):\n",
    "            self.state[j] -= summ * torch.tensor(action[2], dtype=torch.float32)\n",
    "\n",
    "        # Example: Compute reward and done flag\n",
    "        reward = -1\n",
    "        done = torch.equal(self.state, torch.zeros(self.m * self.n, self.n * self.d, self.m * self.d))\n",
    "\n",
    "        return self.state, reward, done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.106146Z",
     "end_time": "2023-06-15T14:01:34.843407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class BatchBuffer:\n",
    "  def __init__(self, memory_size=1000):\n",
    "    self.memory_size = memory_size\n",
    "    self.Buffer = namedtuple('Buffer', 'state action reward next_state done')\n",
    "    self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "  def get_batch(self): # to change\n",
    "    rand_ind = np.random.choice(len(self.replay_memory))\n",
    "    return self.replay_memory[rand_ind]\n",
    "\n",
    "  def append(self, state, action, reward, next_state, done):\n",
    "    self.replay_memory.append(self.Buffer(state, action, reward, next_state, done))\n",
    "\n",
    "  def append_batch(self, state_batch, action_batch, reward_batch, next_state_batch, done):\n",
    "    for i in range(state_batch.shape[0]):\n",
    "      self.append(state_batch[i], action_batch[i], reward_batch[i], next_state_batch[i], done)\n",
    "\n",
    "  def clear(self):\n",
    "    self.replay_memory.clear()\n",
    "\n",
    "  def print_len_buffer(self):\n",
    "    return len(self.replay_memory)\n",
    "\n",
    "env_shape = (2, 2, 2)\n",
    "env = Environment(env_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.109383Z",
     "end_time": "2023-06-15T14:01:34.846771Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_size, init_w=3e-1):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state = self.state_rep(state)\n",
    "        x = F.tanh(self.linear1(state))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        # x = torch.tanh(self.linear3(x)) # in case embeds are -1 1 normalized\n",
    "        x = torch.tanh(self.linear3(x)) # in case embeds are standard scaled / wiped using PCA whitening\n",
    "        # return state, x\n",
    "        return x # action"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.112148Z",
     "end_time": "2023-06-15T14:01:34.849795Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_size, init_w=3e-5):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim + action_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=0)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.115348Z",
     "end_time": "2023-06-15T14:01:34.853078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def soft_update(net, target_net, soft_tau=1e-2):\n",
    "    for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.118027Z",
     "end_time": "2023-06-15T14:01:34.856071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def ddpg_update(step_batch, params, learn=True, step=-1):\n",
    "\n",
    "    state, action, reward, next_state, done = step_batch\n",
    "\n",
    "    # --------------------------------------------------------#\n",
    "    # Value Learning\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_action = torch.Tensor(select_action(next_state, target_policy_net))\n",
    "        target_value   = target_value_net(next_state.resize(64), next_action.detach().resize(12))\n",
    "        expected_value = reward + (1.0 - done) * params['gamma'] * target_value\n",
    "        expected_value = torch.clamp(expected_value,\n",
    "                                     params['min_value'], params['max_value'])\n",
    "\n",
    "    value = value_net(state.resize(64), torch.Tensor(action).resize(12))\n",
    "\n",
    "    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n",
    "\n",
    "    if learn:\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "    else:\n",
    "        debug['next_action'] = next_action\n",
    "        writer.add_figure('next_action',\n",
    "                    misc.pairwise_distances_fig(next_action[:50]), step)\n",
    "        writer.add_histogram('value', value, step)\n",
    "        writer.add_histogram('target_value', target_value, step)\n",
    "        writer.add_histogram('expected_value', expected_value, step)\n",
    "\n",
    "    # --------------------------------------------------------#\n",
    "    # Policy learning\n",
    "\n",
    "    gen_action = select_action(state, policy_net)\n",
    "    policy_loss = -value_net(state.resize(64), torch.from_numpy(gen_action).resize(12))\n",
    "\n",
    "    if not learn:\n",
    "        debug['gen_action'] = gen_action\n",
    "        writer.add_histogram('policy_loss', policy_loss, step)\n",
    "        writer.add_figure('next_action',\n",
    "                    misc.pairwise_distances_fig(gen_action[:50]), step)\n",
    "\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    if learn and step % params['policy_step']== 0:\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy_net.parameters(), -1, 1)\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        soft_update(value_net, target_value_net, soft_tau=params['soft_tau'])\n",
    "        soft_update(policy_net, target_policy_net, soft_tau=params['soft_tau'])\n",
    "\n",
    "\n",
    "    losses = {'value': value_loss.item(), 'policy': policy_loss.item(), 'step': step}\n",
    "    misc.write_losses(writer, losses, kind='train' if learn else 'test')\n",
    "    return losses\n",
    "\n",
    "# === ddpg settings ===\n",
    "\n",
    "params = {\n",
    "    'gamma'      : 0.99,\n",
    "    'min_value'  : -10,\n",
    "    'max_value'  : 10,\n",
    "    'policy_step': 10,\n",
    "    'soft_tau'   : 0.001,\n",
    "    'max_steps'  : 8,\n",
    "    'policy_lr'  : 1e-5,\n",
    "    'value_lr'   : 1e-5,\n",
    "    'actor_weight_init': 54e-2,\n",
    "    'critic_weight_init': 6e-1,\n",
    "}\n",
    "\n",
    "# === end ===\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.120153Z",
     "end_time": "2023-06-15T14:01:34.858292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "value_net  = Critic(64, 12, 128, params['critic_weight_init']).to(cuda)\n",
    "policy_net = Actor(64, 12, 128, params['actor_weight_init']).to(cuda)\n",
    "\n",
    "\n",
    "target_value_net = Critic(64, 12, 128).to(cuda)\n",
    "target_policy_net = Actor(64, 12, 128).to(cuda)\n",
    "\n",
    "\n",
    "target_policy_net.eval()\n",
    "target_value_net.eval()\n",
    "\n",
    "soft_update(value_net, target_value_net, soft_tau=1.0)\n",
    "soft_update(policy_net, target_policy_net, soft_tau=1.0)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "\n",
    "# from good to bad: Ranger Radam Adam RMSprop\n",
    "value_optimizer = optim.Ranger(value_net.parameters(),\n",
    "                              lr=params['value_lr'], weight_decay=1e-2)\n",
    "policy_optimizer = optim.Ranger(policy_net.parameters(),\n",
    "                               lr=params['policy_lr'], weight_decay=1e-5)\n",
    "\n",
    "loss = {\n",
    "    'test': {'value': [], 'policy': [], 'step': []},\n",
    "    'train': {'value': [], 'policy': [], 'step': []}\n",
    "    }\n",
    "\n",
    "debug = {}\n",
    "\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "plotter = misc.Plotter(loss, [['value', 'policy']],)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.124938Z",
     "end_time": "2023-06-15T14:01:34.862966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def select_action(state, actor):\n",
    "        state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "        action = actor(state).detach().numpy()[0]\n",
    "        action = np.array(tuple(map(lambda el: max((-1 if -1<=el<-0.33 else 0, 1 if 0.33<el<=1 else 0), key=abs), action))).reshape(3, 4)\n",
    "        return action\n",
    "\n",
    "act = select_action(env.reset(), policy_net)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.131916Z",
     "end_time": "2023-06-15T14:01:34.872765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/7g_7wfqd5hj9qmks2g8q5l200000gn/T/ipykernel_49000/3093035418.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
      "/var/folders/y7/7g_7wfqd5hj9qmks2g8q5l200000gn/T/ipykernel_49000/3093035418.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n"
     ]
    }
   ],
   "source": [
    "buffer = BatchBuffer()\n",
    "env = Environment(env_shape)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "for i in range(num_episodes):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  episode_reward = 0\n",
    "  steps = 0\n",
    "\n",
    "  while not done:\n",
    "    if steps == params['max_steps']:\n",
    "      episode_reward -= params['max_steps']\n",
    "      buffer.append(state, action, episode_reward, next_state, done)\n",
    "      break\n",
    "    action = select_action(state, policy_net)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    buffer.append(state, action, episode_reward, next_state, done)\n",
    "\n",
    "    state = torch.clone(next_state)\n",
    "    steps += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:42.135407Z",
     "end_time": "2023-06-15T14:01:34.873536Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/7g_7wfqd5hj9qmks2g8q5l200000gn/T/ipykernel_49000/3093035418.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
      "/Users/artemvolosevich/Documents/JetProjects/PyProjects/DPR-Matrix-Multiplication/venv/lib/python3.11/site-packages/torch/_tensor.py:775: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = buffer.get_batch()\n",
    "ddpg_update(batch, params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:59:41.674225Z",
     "end_time": "2023-06-15T14:01:34.419764Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
